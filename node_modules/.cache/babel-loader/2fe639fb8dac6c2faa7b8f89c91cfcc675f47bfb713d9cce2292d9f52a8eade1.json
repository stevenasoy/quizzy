{"ast":null,"code":"import _toConsumableArray from \"C:/Users/StevenJakeASOY/Documents/cursor-projects/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _slicedToArray from \"C:/Users/StevenJakeASOY/Documents/cursor-projects/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _objectSpread from \"C:/Users/StevenJakeASOY/Documents/cursor-projects/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\nimport \"core-js/modules/es.array.concat.js\";\nimport \"core-js/modules/es.array.filter.js\";\nimport \"core-js/modules/es.array.for-each.js\";\nimport \"core-js/modules/es.array.from.js\";\nimport \"core-js/modules/es.array.includes.js\";\nimport \"core-js/modules/es.array.index-of.js\";\nimport \"core-js/modules/es.array.map.js\";\nimport \"core-js/modules/es.array.push.js\";\nimport \"core-js/modules/es.array.reduce.js\";\nimport \"core-js/modules/es.array.some.js\";\nimport \"core-js/modules/es.iterator.constructor.js\";\nimport \"core-js/modules/es.iterator.filter.js\";\nimport \"core-js/modules/es.iterator.for-each.js\";\nimport \"core-js/modules/es.iterator.map.js\";\nimport \"core-js/modules/es.iterator.reduce.js\";\nimport \"core-js/modules/es.iterator.some.js\";\nimport \"core-js/modules/es.map.js\";\nimport \"core-js/modules/es.object.to-string.js\";\nimport \"core-js/modules/es.object.values.js\";\nimport \"core-js/modules/es.regexp.constructor.js\";\nimport \"core-js/modules/es.regexp.dot-all.js\";\nimport \"core-js/modules/es.regexp.exec.js\";\nimport \"core-js/modules/es.regexp.sticky.js\";\nimport \"core-js/modules/es.regexp.to-string.js\";\nimport \"core-js/modules/es.set.js\";\nimport \"core-js/modules/es.set.difference.v2.js\";\nimport \"core-js/modules/es.set.intersection.v2.js\";\nimport \"core-js/modules/es.set.is-disjoint-from.v2.js\";\nimport \"core-js/modules/es.set.is-subset-of.v2.js\";\nimport \"core-js/modules/es.set.is-superset-of.v2.js\";\nimport \"core-js/modules/es.set.symmetric-difference.v2.js\";\nimport \"core-js/modules/es.set.union.v2.js\";\nimport \"core-js/modules/es.string.includes.js\";\nimport \"core-js/modules/es.string.iterator.js\";\nimport \"core-js/modules/es.string.match.js\";\nimport \"core-js/modules/es.string.split.js\";\nimport \"core-js/modules/es.string.substr.js\";\nimport \"core-js/modules/es.string.trim.js\";\nimport \"core-js/modules/web.dom-collections.for-each.js\";\nimport \"core-js/modules/web.dom-collections.iterator.js\";\n/**\r\n * Content Processing Algorithms Module\r\n */\n\n/**\r\n * Scores term frequency in content\r\n * @param {string} content - Content to analyze\r\n * @param {string[]} terms - Terms to look for\r\n * @returns {Object} Term frequency scores\r\n */\nexport function scoreTermFrequency(content, terms) {\n  var scores = {};\n  var normalizedContent = content.toLowerCase();\n  terms.forEach(function (term) {\n    var regex = new RegExp(\"\\\\b\".concat(term, \"\\\\b\"), 'g');\n    var matches = normalizedContent.match(regex);\n    scores[term] = matches ? matches.length : 0;\n  });\n  return scores;\n}\n\n/**\r\n * Finds exact phrase matches in content\r\n * @param {string} content - Content to search in\r\n * @param {string} phrase - Phrase to match\r\n * @returns {Array} Array of matches with their positions\r\n */\nexport function findExactPhraseMatches(content, phrase) {\n  var matches = [];\n  var pos = content.toLowerCase().indexOf(phrase.toLowerCase());\n  while (pos !== -1) {\n    matches.push({\n      position: pos,\n      context: content.substr(Math.max(0, pos - 50), 100)\n    });\n    pos = content.toLowerCase().indexOf(phrase.toLowerCase(), pos + 1);\n  }\n  return matches;\n}\n\n/**\r\n * Ranks content relevance based on multiple factors\r\n * @param {string} content - Content to analyze\r\n * @param {Object} criteria - Ranking criteria\r\n * @returns {number} Relevance score\r\n */\nexport function rankContentRelevance(content, criteria) {\n  var relevanceScore = 0;\n\n  // Score exact phrase matches\n  if (criteria.phrase) {\n    var exactMatches = findExactPhraseMatches(content, criteria.phrase);\n    relevanceScore += exactMatches.length * 15;\n  }\n\n  // Score keyword matches\n  if (criteria.keywords) {\n    var termScores = scoreTermFrequency(content, criteria.keywords);\n    relevanceScore += Object.values(termScores).reduce(function (a, b) {\n      return a + b;\n    }, 0) * 2;\n  }\n  return Math.min(relevanceScore, 100);\n}\n\n/**\r\n * Analyzes content-to-question ratio\r\n * @param {string} content - Content to analyze\r\n * @param {number} questionCount - Number of questions\r\n * @returns {Object} Analysis results\r\n */\nexport function analyzeContentQuestionRatio(content, questionCount) {\n  var contentLength = content.length;\n  var averageContentPerQuestion = 500; // baseline characters per question\n  var expectedQuestions = Math.ceil(contentLength / averageContentPerQuestion);\n  return {\n    contentLength: contentLength,\n    expectedQuestions: expectedQuestions,\n    actualQuestions: questionCount,\n    ratio: questionCount / expectedQuestions,\n    isAdequate: questionCount <= expectedQuestions\n  };\n}\n\n/**\r\n * Estimates content coverage using semantic analysis\r\n * @param {string} content - Content to analyze\r\n * @param {Array} questions - Array of questions\r\n * @returns {Object} Coverage analysis\r\n */\nexport function estimateContentCoverage(content, questions) {\n  // Split content into meaningful chunks\n  var chunks = splitIntoChunks(content);\n\n  // Extract key concepts from content\n  var contentConcepts = extractKeyConcepts(content);\n\n  // Create topic clusters\n  var topicClusters = createTopicClusters(chunks);\n\n  // Track coverage for each cluster and content concepts\n  var clusterCoverage = new Map();\n  var conceptCoverage = new Set();\n  topicClusters.forEach(function (cluster, index) {\n    clusterCoverage.set(index, {\n      covered: false,\n      relevantQuestions: [],\n      concepts: cluster.concepts,\n      contentConcepts: contentConcepts.filter(function (concept) {\n        return cluster.concepts.some(function (clusterConcept) {\n          return calculateSemanticSimilarity([clusterConcept], [concept]) > 0.6;\n        });\n      })\n    });\n  });\n\n  // Analyze each question's coverage\n  questions.forEach(function (question) {\n    var questionConcepts = extractKeyConcepts(question.text);\n    topicClusters.forEach(function (cluster, index) {\n      var semanticSimilarity = calculateSemanticSimilarity(questionConcepts, cluster.concepts);\n      if (semanticSimilarity > 0.6) {\n        // Threshold for considering content covered\n        var coverage = clusterCoverage.get(index);\n        coverage.covered = true;\n        coverage.relevantQuestions.push({\n          questionId: question.id,\n          similarity: semanticSimilarity\n        });\n      }\n    });\n  });\n\n  // Calculate coverage metrics\n  var coverageMetrics = calculateCoverageMetrics(clusterCoverage);\n  return _objectSpread(_objectSpread({}, coverageMetrics), {}, {\n    topicClusters: Array.from(clusterCoverage.entries()).map(function (_ref) {\n      var _ref2 = _slicedToArray(_ref, 2),\n        index = _ref2[0],\n        data = _ref2[1];\n      return {\n        id: index,\n        concepts: data.concepts,\n        covered: data.covered,\n        questions: data.relevantQuestions\n      };\n    })\n  });\n}\n\n/**\r\n * Splits content into meaningful chunks\r\n * @param {string} content - Content to split\r\n * @returns {Array} Content chunks\r\n */\nfunction splitIntoChunks(content) {\n  // Split by paragraphs first\n  var paragraphs = content.split(/\\n\\s*\\n/);\n\n  // Further split long paragraphs\n  var chunks = [];\n  paragraphs.forEach(function (paragraph) {\n    if (paragraph.length > 500) {\n      // Split by sentences for long paragraphs\n      var sentences = paragraph.split(/[.!?]+/);\n      var currentChunk = '';\n      sentences.forEach(function (sentence) {\n        if (currentChunk.length + sentence.length > 500) {\n          if (currentChunk) chunks.push(currentChunk.trim());\n          currentChunk = sentence;\n        } else {\n          currentChunk += ' ' + sentence;\n        }\n      });\n      if (currentChunk) chunks.push(currentChunk.trim());\n    } else {\n      chunks.push(paragraph.trim());\n    }\n  });\n  return chunks.filter(function (chunk) {\n    return chunk.length > 0;\n  });\n}\n\n/**\r\n * Extracts key concepts from text\r\n * @param {string} text - Text to analyze\r\n * @returns {Array} Key concepts\r\n */\nfunction extractKeyConcepts(text) {\n  var concepts = new Set();\n\n  // Extract noun phrases using regex patterns\n  var nounPhrasePatterns = [/\\b([A-Z][a-z]+\\s+)+[A-Z][a-z]+\\b/,\n  // Proper noun phrases\n  /\\b[A-Z][a-z]+\\b/,\n  // Single proper nouns\n  /\\b[a-z]+\\s+(?:of|in|for|to|with)\\s+[a-z]+\\b/g,\n  // Prepositional phrases\n  /\\b[a-z]+ing\\s+[a-z]+\\b/g // Gerund phrases\n  ];\n  nounPhrasePatterns.forEach(function (pattern) {\n    var matches = text.match(pattern) || [];\n    matches.forEach(function (match) {\n      return concepts.add(match.toLowerCase());\n    });\n  });\n\n  // Extract technical terms\n  technicalTerms.forEach(function (term) {\n    if (text.toLowerCase().includes(term)) {\n      concepts.add(term);\n    }\n  });\n  return Array.from(concepts);\n}\n\n/**\r\n * Creates topic clusters from content chunks\r\n * @param {Array} chunks - Content chunks\r\n * @returns {Array} Topic clusters\r\n */\nfunction createTopicClusters(chunks) {\n  var clusters = [];\n  var processedChunks = new Set();\n  chunks.forEach(function (chunk, i) {\n    if (processedChunks.has(i)) return;\n    var cluster = {\n      chunks: [chunk],\n      concepts: extractKeyConcepts(chunk)\n    };\n\n    // Find related chunks\n    chunks.forEach(function (otherChunk, j) {\n      if (i !== j && !processedChunks.has(j)) {\n        var similarity = calculateSemanticSimilarity(extractKeyConcepts(chunk), extractKeyConcepts(otherChunk));\n        if (similarity > 0.5) {\n          cluster.chunks.push(otherChunk);\n          cluster.concepts = _toConsumableArray(new Set([].concat(_toConsumableArray(cluster.concepts), _toConsumableArray(extractKeyConcepts(otherChunk)))));\n          processedChunks.add(j);\n        }\n      }\n    });\n    clusters.push(cluster);\n    processedChunks.add(i);\n  });\n  return clusters;\n}\n\n/**\r\n * Calculates semantic similarity between concept sets\r\n * @param {Array} concepts1 - First set of concepts\r\n * @param {Array} concepts2 - Second set of concepts\r\n * @returns {number} Similarity score (0-1)\r\n */\nfunction calculateSemanticSimilarity(concepts1, concepts2) {\n  var set1 = new Set(concepts1);\n  var set2 = new Set(concepts2);\n\n  // Calculate Jaccard similarity\n  var intersection = new Set(_toConsumableArray(set1).filter(function (x) {\n    return set2.has(x);\n  }));\n  var union = new Set([].concat(_toConsumableArray(set1), _toConsumableArray(set2)));\n\n  // Add synonym matching\n  var synonymScore = 0;\n  concepts1.forEach(function (concept1) {\n    concepts2.forEach(function (concept2) {\n      if (areSynonyms(concept1, concept2)) {\n        synonymScore += 1;\n      }\n    });\n  });\n  var jaccardSimilarity = intersection.size / union.size;\n  var normalizedSynonymScore = synonymScore / Math.max(concepts1.length, concepts2.length);\n  return jaccardSimilarity * 0.7 + normalizedSynonymScore * 0.3;\n}\n\n/**\r\n * Checks if two terms are synonyms\r\n * @param {string} term1 - First term\r\n * @param {string} term2 - Second term\r\n * @returns {boolean} Whether terms are synonyms\r\n */\nfunction areSynonyms(term1, term2) {\n  return synonymSets.some(function (set) {\n    return set.has(term1.toLowerCase()) && set.has(term2.toLowerCase());\n  });\n}\n\n/**\r\n * Calculates coverage metrics\r\n * @param {Map} clusterCoverage - Coverage data for each cluster\r\n * @returns {Object} Coverage metrics\r\n */\nfunction calculateCoverageMetrics(clusterCoverage) {\n  var totalClusters = clusterCoverage.size;\n  var coveredClusters = Array.from(clusterCoverage.values()).filter(function (cluster) {\n    return cluster.covered;\n  }).length;\n  var coveragePercentage = coveredClusters / totalClusters * 100;\n\n  // Calculate distribution of questions across clusters\n  var questionDistribution = Array.from(clusterCoverage.values()).map(function (cluster) {\n    return cluster.relevantQuestions.length;\n  });\n  var avgQuestionsPerCluster = questionDistribution.reduce(function (a, b) {\n    return a + b;\n  }, 0) / totalClusters;\n  var stdDevQuestions = calculateStandardDeviation(questionDistribution);\n  return {\n    totalClusters: totalClusters,\n    coveredClusters: coveredClusters,\n    coveragePercentage: coveragePercentage,\n    distribution: {\n      avgQuestionsPerCluster: avgQuestionsPerCluster,\n      stdDevQuestions: stdDevQuestions,\n      isBalanced: stdDevQuestions < avgQuestionsPerCluster * 0.5\n    }\n  };\n}\n\n/**\r\n * Calculates standard deviation\r\n * @param {Array} values - Array of numbers\r\n * @returns {number} Standard deviation\r\n */\nfunction calculateStandardDeviation(values) {\n  var avg = values.reduce(function (a, b) {\n    return a + b;\n  }, 0) / values.length;\n  var squareDiffs = values.map(function (value) {\n    return Math.pow(value - avg, 2);\n  });\n  return Math.sqrt(squareDiffs.reduce(function (a, b) {\n    return a + b;\n  }, 0) / values.length);\n}\n\n// Technical terms and synonyms data\nvar technicalTerms = new Set(['algorithm', 'analysis', 'framework', 'methodology', 'implementation', 'architecture', 'infrastructure', 'optimization']);\nvar synonymSets = [new Set(['increase', 'rise', 'grow', 'expand']), new Set(['decrease', 'decline', 'reduce', 'shrink']), new Set(['analyze', 'examine', 'investigate', 'study']), new Set(['create', 'generate', 'produce', 'develop'])\n// Add more synonym sets as needed\n];","map":{"version":3,"names":["scoreTermFrequency","content","terms","scores","normalizedContent","toLowerCase","forEach","term","regex","RegExp","concat","matches","match","length","findExactPhraseMatches","phrase","pos","indexOf","push","position","context","substr","Math","max","rankContentRelevance","criteria","relevanceScore","exactMatches","keywords","termScores","Object","values","reduce","a","b","min","analyzeContentQuestionRatio","questionCount","contentLength","averageContentPerQuestion","expectedQuestions","ceil","actualQuestions","ratio","isAdequate","estimateContentCoverage","questions","chunks","splitIntoChunks","contentConcepts","extractKeyConcepts","topicClusters","createTopicClusters","clusterCoverage","Map","conceptCoverage","Set","cluster","index","set","covered","relevantQuestions","concepts","filter","concept","some","clusterConcept","calculateSemanticSimilarity","question","questionConcepts","text","semanticSimilarity","coverage","get","questionId","id","similarity","coverageMetrics","calculateCoverageMetrics","_objectSpread","Array","from","entries","map","_ref","_ref2","_slicedToArray","data","paragraphs","split","paragraph","sentences","currentChunk","sentence","trim","chunk","nounPhrasePatterns","pattern","add","technicalTerms","includes","clusters","processedChunks","i","has","otherChunk","j","_toConsumableArray","concepts1","concepts2","set1","set2","intersection","x","union","synonymScore","concept1","concept2","areSynonyms","jaccardSimilarity","size","normalizedSynonymScore","term1","term2","synonymSets","totalClusters","coveredClusters","coveragePercentage","questionDistribution","avgQuestionsPerCluster","stdDevQuestions","calculateStandardDeviation","distribution","isBalanced","avg","squareDiffs","value","pow","sqrt"],"sources":["C:/Users/StevenJakeASOY/Documents/cursor-projects/src/algorithms/content-processing/index.js"],"sourcesContent":["/**\r\n * Content Processing Algorithms Module\r\n */\r\n\r\n/**\r\n * Scores term frequency in content\r\n * @param {string} content - Content to analyze\r\n * @param {string[]} terms - Terms to look for\r\n * @returns {Object} Term frequency scores\r\n */\r\nexport function scoreTermFrequency(content, terms) {\r\n  const scores = {};\r\n  const normalizedContent = content.toLowerCase();\r\n  \r\n  terms.forEach(term => {\r\n    const regex = new RegExp(`\\\\b${term}\\\\b`, 'g');\r\n    const matches = normalizedContent.match(regex);\r\n    scores[term] = matches ? matches.length : 0;\r\n  });\r\n  \r\n  return scores;\r\n}\r\n\r\n/**\r\n * Finds exact phrase matches in content\r\n * @param {string} content - Content to search in\r\n * @param {string} phrase - Phrase to match\r\n * @returns {Array} Array of matches with their positions\r\n */\r\nexport function findExactPhraseMatches(content, phrase) {\r\n  const matches = [];\r\n  let pos = content.toLowerCase().indexOf(phrase.toLowerCase());\r\n  \r\n  while (pos !== -1) {\r\n    matches.push({\r\n      position: pos,\r\n      context: content.substr(Math.max(0, pos - 50), 100)\r\n    });\r\n    pos = content.toLowerCase().indexOf(phrase.toLowerCase(), pos + 1);\r\n  }\r\n  \r\n  return matches;\r\n}\r\n\r\n/**\r\n * Ranks content relevance based on multiple factors\r\n * @param {string} content - Content to analyze\r\n * @param {Object} criteria - Ranking criteria\r\n * @returns {number} Relevance score\r\n */\r\nexport function rankContentRelevance(content, criteria) {\r\n  let relevanceScore = 0;\r\n  \r\n  // Score exact phrase matches\r\n  if (criteria.phrase) {\r\n    const exactMatches = findExactPhraseMatches(content, criteria.phrase);\r\n    relevanceScore += exactMatches.length * 15;\r\n  }\r\n  \r\n  // Score keyword matches\r\n  if (criteria.keywords) {\r\n    const termScores = scoreTermFrequency(content, criteria.keywords);\r\n    relevanceScore += Object.values(termScores).reduce((a, b) => a + b, 0) * 2;\r\n  }\r\n  \r\n  return Math.min(relevanceScore, 100);\r\n}\r\n\r\n/**\r\n * Analyzes content-to-question ratio\r\n * @param {string} content - Content to analyze\r\n * @param {number} questionCount - Number of questions\r\n * @returns {Object} Analysis results\r\n */\r\nexport function analyzeContentQuestionRatio(content, questionCount) {\r\n  const contentLength = content.length;\r\n  const averageContentPerQuestion = 500; // baseline characters per question\r\n  const expectedQuestions = Math.ceil(contentLength / averageContentPerQuestion);\r\n  \r\n  return {\r\n    contentLength,\r\n    expectedQuestions,\r\n    actualQuestions: questionCount,\r\n    ratio: questionCount / expectedQuestions,\r\n    isAdequate: questionCount <= expectedQuestions\r\n  };\r\n}\r\n\r\n/**\r\n * Estimates content coverage using semantic analysis\r\n * @param {string} content - Content to analyze\r\n * @param {Array} questions - Array of questions\r\n * @returns {Object} Coverage analysis\r\n */\r\nexport function estimateContentCoverage(content, questions) {\r\n  // Split content into meaningful chunks\r\n  const chunks = splitIntoChunks(content);\r\n  \r\n  // Extract key concepts from content\r\n  const contentConcepts = extractKeyConcepts(content);\r\n  \r\n  // Create topic clusters\r\n  const topicClusters = createTopicClusters(chunks);\r\n  \r\n  // Track coverage for each cluster and content concepts\r\n  const clusterCoverage = new Map();\r\n  const conceptCoverage = new Set();\r\n  \r\n  topicClusters.forEach((cluster, index) => {\r\n    clusterCoverage.set(index, {\r\n      covered: false,\r\n      relevantQuestions: [],\r\n      concepts: cluster.concepts,\r\n      contentConcepts: contentConcepts.filter(concept => \r\n        cluster.concepts.some(clusterConcept => \r\n          calculateSemanticSimilarity([clusterConcept], [concept]) > 0.6\r\n        )\r\n      )\r\n    });\r\n  });\r\n\r\n  // Analyze each question's coverage\r\n  questions.forEach(question => {\r\n    const questionConcepts = extractKeyConcepts(question.text);\r\n    \r\n    topicClusters.forEach((cluster, index) => {\r\n      const semanticSimilarity = calculateSemanticSimilarity(\r\n        questionConcepts,\r\n        cluster.concepts\r\n      );\r\n\r\n      if (semanticSimilarity > 0.6) { // Threshold for considering content covered\r\n        const coverage = clusterCoverage.get(index);\r\n        coverage.covered = true;\r\n        coverage.relevantQuestions.push({\r\n          questionId: question.id,\r\n          similarity: semanticSimilarity\r\n        });\r\n      }\r\n    });\r\n  });\r\n\r\n  // Calculate coverage metrics\r\n  const coverageMetrics = calculateCoverageMetrics(clusterCoverage);\r\n\r\n  return {\r\n    ...coverageMetrics,\r\n    topicClusters: Array.from(clusterCoverage.entries()).map(([index, data]) => ({\r\n      id: index,\r\n      concepts: data.concepts,\r\n      covered: data.covered,\r\n      questions: data.relevantQuestions\r\n    }))\r\n  };\r\n}\r\n\r\n/**\r\n * Splits content into meaningful chunks\r\n * @param {string} content - Content to split\r\n * @returns {Array} Content chunks\r\n */\r\nfunction splitIntoChunks(content) {\r\n  // Split by paragraphs first\r\n  const paragraphs = content.split(/\\n\\s*\\n/);\r\n  \r\n  // Further split long paragraphs\r\n  const chunks = [];\r\n  paragraphs.forEach(paragraph => {\r\n    if (paragraph.length > 500) {\r\n      // Split by sentences for long paragraphs\r\n      const sentences = paragraph.split(/[.!?]+/);\r\n      let currentChunk = '';\r\n      \r\n      sentences.forEach(sentence => {\r\n        if (currentChunk.length + sentence.length > 500) {\r\n          if (currentChunk) chunks.push(currentChunk.trim());\r\n          currentChunk = sentence;\r\n        } else {\r\n          currentChunk += ' ' + sentence;\r\n        }\r\n      });\r\n      \r\n      if (currentChunk) chunks.push(currentChunk.trim());\r\n    } else {\r\n      chunks.push(paragraph.trim());\r\n    }\r\n  });\r\n  \r\n  return chunks.filter(chunk => chunk.length > 0);\r\n}\r\n\r\n/**\r\n * Extracts key concepts from text\r\n * @param {string} text - Text to analyze\r\n * @returns {Array} Key concepts\r\n */\r\nfunction extractKeyConcepts(text) {\r\n  const concepts = new Set();\r\n  \r\n  // Extract noun phrases using regex patterns\r\n  const nounPhrasePatterns = [\r\n    /\\b([A-Z][a-z]+\\s+)+[A-Z][a-z]+\\b/, // Proper noun phrases\r\n    /\\b[A-Z][a-z]+\\b/, // Single proper nouns\r\n    /\\b[a-z]+\\s+(?:of|in|for|to|with)\\s+[a-z]+\\b/g, // Prepositional phrases\r\n    /\\b[a-z]+ing\\s+[a-z]+\\b/g, // Gerund phrases\r\n  ];\r\n\r\n  nounPhrasePatterns.forEach(pattern => {\r\n    const matches = text.match(pattern) || [];\r\n    matches.forEach(match => concepts.add(match.toLowerCase()));\r\n  });\r\n\r\n  // Extract technical terms\r\n  technicalTerms.forEach(term => {\r\n    if (text.toLowerCase().includes(term)) {\r\n      concepts.add(term);\r\n    }\r\n  });\r\n\r\n  return Array.from(concepts);\r\n}\r\n\r\n/**\r\n * Creates topic clusters from content chunks\r\n * @param {Array} chunks - Content chunks\r\n * @returns {Array} Topic clusters\r\n */\r\nfunction createTopicClusters(chunks) {\r\n  const clusters = [];\r\n  const processedChunks = new Set();\r\n\r\n  chunks.forEach((chunk, i) => {\r\n    if (processedChunks.has(i)) return;\r\n\r\n    const cluster = {\r\n      chunks: [chunk],\r\n      concepts: extractKeyConcepts(chunk)\r\n    };\r\n\r\n    // Find related chunks\r\n    chunks.forEach((otherChunk, j) => {\r\n      if (i !== j && !processedChunks.has(j)) {\r\n        const similarity = calculateSemanticSimilarity(\r\n          extractKeyConcepts(chunk),\r\n          extractKeyConcepts(otherChunk)\r\n        );\r\n\r\n        if (similarity > 0.5) {\r\n          cluster.chunks.push(otherChunk);\r\n          cluster.concepts = [...new Set([\r\n            ...cluster.concepts,\r\n            ...extractKeyConcepts(otherChunk)\r\n          ])];\r\n          processedChunks.add(j);\r\n        }\r\n      }\r\n    });\r\n\r\n    clusters.push(cluster);\r\n    processedChunks.add(i);\r\n  });\r\n\r\n  return clusters;\r\n}\r\n\r\n/**\r\n * Calculates semantic similarity between concept sets\r\n * @param {Array} concepts1 - First set of concepts\r\n * @param {Array} concepts2 - Second set of concepts\r\n * @returns {number} Similarity score (0-1)\r\n */\r\nfunction calculateSemanticSimilarity(concepts1, concepts2) {\r\n  const set1 = new Set(concepts1);\r\n  const set2 = new Set(concepts2);\r\n\r\n  // Calculate Jaccard similarity\r\n  const intersection = new Set([...set1].filter(x => set2.has(x)));\r\n  const union = new Set([...set1, ...set2]);\r\n\r\n  // Add synonym matching\r\n  let synonymScore = 0;\r\n  concepts1.forEach(concept1 => {\r\n    concepts2.forEach(concept2 => {\r\n      if (areSynonyms(concept1, concept2)) {\r\n        synonymScore += 1;\r\n      }\r\n    });\r\n  });\r\n\r\n  const jaccardSimilarity = intersection.size / union.size;\r\n  const normalizedSynonymScore = synonymScore / Math.max(concepts1.length, concepts2.length);\r\n\r\n  return (jaccardSimilarity * 0.7) + (normalizedSynonymScore * 0.3);\r\n}\r\n\r\n/**\r\n * Checks if two terms are synonyms\r\n * @param {string} term1 - First term\r\n * @param {string} term2 - Second term\r\n * @returns {boolean} Whether terms are synonyms\r\n */\r\nfunction areSynonyms(term1, term2) {\r\n  return synonymSets.some(set => \r\n    set.has(term1.toLowerCase()) && set.has(term2.toLowerCase())\r\n  );\r\n}\r\n\r\n/**\r\n * Calculates coverage metrics\r\n * @param {Map} clusterCoverage - Coverage data for each cluster\r\n * @returns {Object} Coverage metrics\r\n */\r\nfunction calculateCoverageMetrics(clusterCoverage) {\r\n  const totalClusters = clusterCoverage.size;\r\n  const coveredClusters = Array.from(clusterCoverage.values())\r\n    .filter(cluster => cluster.covered).length;\r\n\r\n  const coveragePercentage = (coveredClusters / totalClusters) * 100;\r\n  \r\n  // Calculate distribution of questions across clusters\r\n  const questionDistribution = Array.from(clusterCoverage.values())\r\n    .map(cluster => cluster.relevantQuestions.length);\r\n  \r\n  const avgQuestionsPerCluster = questionDistribution.reduce((a, b) => a + b, 0) / totalClusters;\r\n  const stdDevQuestions = calculateStandardDeviation(questionDistribution);\r\n\r\n  return {\r\n    totalClusters,\r\n    coveredClusters,\r\n    coveragePercentage,\r\n    distribution: {\r\n      avgQuestionsPerCluster,\r\n      stdDevQuestions,\r\n      isBalanced: stdDevQuestions < avgQuestionsPerCluster * 0.5\r\n    }\r\n  };\r\n}\r\n\r\n/**\r\n * Calculates standard deviation\r\n * @param {Array} values - Array of numbers\r\n * @returns {number} Standard deviation\r\n */\r\nfunction calculateStandardDeviation(values) {\r\n  const avg = values.reduce((a, b) => a + b, 0) / values.length;\r\n  const squareDiffs = values.map(value => Math.pow(value - avg, 2));\r\n  return Math.sqrt(squareDiffs.reduce((a, b) => a + b, 0) / values.length);\r\n}\r\n\r\n// Technical terms and synonyms data\r\nconst technicalTerms = new Set([\r\n  'algorithm', 'analysis', 'framework', 'methodology',\r\n  'implementation', 'architecture', 'infrastructure', 'optimization'\r\n]);\r\n\r\nconst synonymSets = [\r\n  new Set(['increase', 'rise', 'grow', 'expand']),\r\n  new Set(['decrease', 'decline', 'reduce', 'shrink']),\r\n  new Set(['analyze', 'examine', 'investigate', 'study']),\r\n  new Set(['create', 'generate', 'produce', 'develop']),\r\n  // Add more synonym sets as needed\r\n]; "],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASA,kBAAkBA,CAACC,OAAO,EAAEC,KAAK,EAAE;EACjD,IAAMC,MAAM,GAAG,CAAC,CAAC;EACjB,IAAMC,iBAAiB,GAAGH,OAAO,CAACI,WAAW,CAAC,CAAC;EAE/CH,KAAK,CAACI,OAAO,CAAC,UAAAC,IAAI,EAAI;IACpB,IAAMC,KAAK,GAAG,IAAIC,MAAM,OAAAC,MAAA,CAAOH,IAAI,UAAO,GAAG,CAAC;IAC9C,IAAMI,OAAO,GAAGP,iBAAiB,CAACQ,KAAK,CAACJ,KAAK,CAAC;IAC9CL,MAAM,CAACI,IAAI,CAAC,GAAGI,OAAO,GAAGA,OAAO,CAACE,MAAM,GAAG,CAAC;EAC7C,CAAC,CAAC;EAEF,OAAOV,MAAM;AACf;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASW,sBAAsBA,CAACb,OAAO,EAAEc,MAAM,EAAE;EACtD,IAAMJ,OAAO,GAAG,EAAE;EAClB,IAAIK,GAAG,GAAGf,OAAO,CAACI,WAAW,CAAC,CAAC,CAACY,OAAO,CAACF,MAAM,CAACV,WAAW,CAAC,CAAC,CAAC;EAE7D,OAAOW,GAAG,KAAK,CAAC,CAAC,EAAE;IACjBL,OAAO,CAACO,IAAI,CAAC;MACXC,QAAQ,EAAEH,GAAG;MACbI,OAAO,EAAEnB,OAAO,CAACoB,MAAM,CAACC,IAAI,CAACC,GAAG,CAAC,CAAC,EAAEP,GAAG,GAAG,EAAE,CAAC,EAAE,GAAG;IACpD,CAAC,CAAC;IACFA,GAAG,GAAGf,OAAO,CAACI,WAAW,CAAC,CAAC,CAACY,OAAO,CAACF,MAAM,CAACV,WAAW,CAAC,CAAC,EAAEW,GAAG,GAAG,CAAC,CAAC;EACpE;EAEA,OAAOL,OAAO;AAChB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASa,oBAAoBA,CAACvB,OAAO,EAAEwB,QAAQ,EAAE;EACtD,IAAIC,cAAc,GAAG,CAAC;;EAEtB;EACA,IAAID,QAAQ,CAACV,MAAM,EAAE;IACnB,IAAMY,YAAY,GAAGb,sBAAsB,CAACb,OAAO,EAAEwB,QAAQ,CAACV,MAAM,CAAC;IACrEW,cAAc,IAAIC,YAAY,CAACd,MAAM,GAAG,EAAE;EAC5C;;EAEA;EACA,IAAIY,QAAQ,CAACG,QAAQ,EAAE;IACrB,IAAMC,UAAU,GAAG7B,kBAAkB,CAACC,OAAO,EAAEwB,QAAQ,CAACG,QAAQ,CAAC;IACjEF,cAAc,IAAII,MAAM,CAACC,MAAM,CAACF,UAAU,CAAC,CAACG,MAAM,CAAC,UAACC,CAAC,EAAEC,CAAC;MAAA,OAAKD,CAAC,GAAGC,CAAC;IAAA,GAAE,CAAC,CAAC,GAAG,CAAC;EAC5E;EAEA,OAAOZ,IAAI,CAACa,GAAG,CAACT,cAAc,EAAE,GAAG,CAAC;AACtC;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASU,2BAA2BA,CAACnC,OAAO,EAAEoC,aAAa,EAAE;EAClE,IAAMC,aAAa,GAAGrC,OAAO,CAACY,MAAM;EACpC,IAAM0B,yBAAyB,GAAG,GAAG,CAAC,CAAC;EACvC,IAAMC,iBAAiB,GAAGlB,IAAI,CAACmB,IAAI,CAACH,aAAa,GAAGC,yBAAyB,CAAC;EAE9E,OAAO;IACLD,aAAa,EAAbA,aAAa;IACbE,iBAAiB,EAAjBA,iBAAiB;IACjBE,eAAe,EAAEL,aAAa;IAC9BM,KAAK,EAAEN,aAAa,GAAGG,iBAAiB;IACxCI,UAAU,EAAEP,aAAa,IAAIG;EAC/B,CAAC;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,SAASK,uBAAuBA,CAAC5C,OAAO,EAAE6C,SAAS,EAAE;EAC1D;EACA,IAAMC,MAAM,GAAGC,eAAe,CAAC/C,OAAO,CAAC;;EAEvC;EACA,IAAMgD,eAAe,GAAGC,kBAAkB,CAACjD,OAAO,CAAC;;EAEnD;EACA,IAAMkD,aAAa,GAAGC,mBAAmB,CAACL,MAAM,CAAC;;EAEjD;EACA,IAAMM,eAAe,GAAG,IAAIC,GAAG,CAAC,CAAC;EACjC,IAAMC,eAAe,GAAG,IAAIC,GAAG,CAAC,CAAC;EAEjCL,aAAa,CAAC7C,OAAO,CAAC,UAACmD,OAAO,EAAEC,KAAK,EAAK;IACxCL,eAAe,CAACM,GAAG,CAACD,KAAK,EAAE;MACzBE,OAAO,EAAE,KAAK;MACdC,iBAAiB,EAAE,EAAE;MACrBC,QAAQ,EAAEL,OAAO,CAACK,QAAQ;MAC1Bb,eAAe,EAAEA,eAAe,CAACc,MAAM,CAAC,UAAAC,OAAO;QAAA,OAC7CP,OAAO,CAACK,QAAQ,CAACG,IAAI,CAAC,UAAAC,cAAc;UAAA,OAClCC,2BAA2B,CAAC,CAACD,cAAc,CAAC,EAAE,CAACF,OAAO,CAAC,CAAC,GAAG,GAAG;QAAA,CAChE,CAAC;MAAA,CACH;IACF,CAAC,CAAC;EACJ,CAAC,CAAC;;EAEF;EACAlB,SAAS,CAACxC,OAAO,CAAC,UAAA8D,QAAQ,EAAI;IAC5B,IAAMC,gBAAgB,GAAGnB,kBAAkB,CAACkB,QAAQ,CAACE,IAAI,CAAC;IAE1DnB,aAAa,CAAC7C,OAAO,CAAC,UAACmD,OAAO,EAAEC,KAAK,EAAK;MACxC,IAAMa,kBAAkB,GAAGJ,2BAA2B,CACpDE,gBAAgB,EAChBZ,OAAO,CAACK,QACV,CAAC;MAED,IAAIS,kBAAkB,GAAG,GAAG,EAAE;QAAE;QAC9B,IAAMC,QAAQ,GAAGnB,eAAe,CAACoB,GAAG,CAACf,KAAK,CAAC;QAC3Cc,QAAQ,CAACZ,OAAO,GAAG,IAAI;QACvBY,QAAQ,CAACX,iBAAiB,CAAC3C,IAAI,CAAC;UAC9BwD,UAAU,EAAEN,QAAQ,CAACO,EAAE;UACvBC,UAAU,EAAEL;QACd,CAAC,CAAC;MACJ;IACF,CAAC,CAAC;EACJ,CAAC,CAAC;;EAEF;EACA,IAAMM,eAAe,GAAGC,wBAAwB,CAACzB,eAAe,CAAC;EAEjE,OAAA0B,aAAA,CAAAA,aAAA,KACKF,eAAe;IAClB1B,aAAa,EAAE6B,KAAK,CAACC,IAAI,CAAC5B,eAAe,CAAC6B,OAAO,CAAC,CAAC,CAAC,CAACC,GAAG,CAAC,UAAAC,IAAA;MAAA,IAAAC,KAAA,GAAAC,cAAA,CAAAF,IAAA;QAAE1B,KAAK,GAAA2B,KAAA;QAAEE,IAAI,GAAAF,KAAA;MAAA,OAAO;QAC3EV,EAAE,EAAEjB,KAAK;QACTI,QAAQ,EAAEyB,IAAI,CAACzB,QAAQ;QACvBF,OAAO,EAAE2B,IAAI,CAAC3B,OAAO;QACrBd,SAAS,EAAEyC,IAAI,CAAC1B;MAClB,CAAC;IAAA,CAAC;EAAC;AAEP;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASb,eAAeA,CAAC/C,OAAO,EAAE;EAChC;EACA,IAAMuF,UAAU,GAAGvF,OAAO,CAACwF,KAAK,CAAC,SAAS,CAAC;;EAE3C;EACA,IAAM1C,MAAM,GAAG,EAAE;EACjByC,UAAU,CAAClF,OAAO,CAAC,UAAAoF,SAAS,EAAI;IAC9B,IAAIA,SAAS,CAAC7E,MAAM,GAAG,GAAG,EAAE;MAC1B;MACA,IAAM8E,SAAS,GAAGD,SAAS,CAACD,KAAK,CAAC,QAAQ,CAAC;MAC3C,IAAIG,YAAY,GAAG,EAAE;MAErBD,SAAS,CAACrF,OAAO,CAAC,UAAAuF,QAAQ,EAAI;QAC5B,IAAID,YAAY,CAAC/E,MAAM,GAAGgF,QAAQ,CAAChF,MAAM,GAAG,GAAG,EAAE;UAC/C,IAAI+E,YAAY,EAAE7C,MAAM,CAAC7B,IAAI,CAAC0E,YAAY,CAACE,IAAI,CAAC,CAAC,CAAC;UAClDF,YAAY,GAAGC,QAAQ;QACzB,CAAC,MAAM;UACLD,YAAY,IAAI,GAAG,GAAGC,QAAQ;QAChC;MACF,CAAC,CAAC;MAEF,IAAID,YAAY,EAAE7C,MAAM,CAAC7B,IAAI,CAAC0E,YAAY,CAACE,IAAI,CAAC,CAAC,CAAC;IACpD,CAAC,MAAM;MACL/C,MAAM,CAAC7B,IAAI,CAACwE,SAAS,CAACI,IAAI,CAAC,CAAC,CAAC;IAC/B;EACF,CAAC,CAAC;EAEF,OAAO/C,MAAM,CAACgB,MAAM,CAAC,UAAAgC,KAAK;IAAA,OAAIA,KAAK,CAAClF,MAAM,GAAG,CAAC;EAAA,EAAC;AACjD;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASqC,kBAAkBA,CAACoB,IAAI,EAAE;EAChC,IAAMR,QAAQ,GAAG,IAAIN,GAAG,CAAC,CAAC;;EAE1B;EACA,IAAMwC,kBAAkB,GAAG,CACzB,kCAAkC;EAAE;EACpC,iBAAiB;EAAE;EACnB,8CAA8C;EAAE;EAChD,yBAAyB,CAAE;EAAA,CAC5B;EAEDA,kBAAkB,CAAC1F,OAAO,CAAC,UAAA2F,OAAO,EAAI;IACpC,IAAMtF,OAAO,GAAG2D,IAAI,CAAC1D,KAAK,CAACqF,OAAO,CAAC,IAAI,EAAE;IACzCtF,OAAO,CAACL,OAAO,CAAC,UAAAM,KAAK;MAAA,OAAIkD,QAAQ,CAACoC,GAAG,CAACtF,KAAK,CAACP,WAAW,CAAC,CAAC,CAAC;IAAA,EAAC;EAC7D,CAAC,CAAC;;EAEF;EACA8F,cAAc,CAAC7F,OAAO,CAAC,UAAAC,IAAI,EAAI;IAC7B,IAAI+D,IAAI,CAACjE,WAAW,CAAC,CAAC,CAAC+F,QAAQ,CAAC7F,IAAI,CAAC,EAAE;MACrCuD,QAAQ,CAACoC,GAAG,CAAC3F,IAAI,CAAC;IACpB;EACF,CAAC,CAAC;EAEF,OAAOyE,KAAK,CAACC,IAAI,CAACnB,QAAQ,CAAC;AAC7B;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASV,mBAAmBA,CAACL,MAAM,EAAE;EACnC,IAAMsD,QAAQ,GAAG,EAAE;EACnB,IAAMC,eAAe,GAAG,IAAI9C,GAAG,CAAC,CAAC;EAEjCT,MAAM,CAACzC,OAAO,CAAC,UAACyF,KAAK,EAAEQ,CAAC,EAAK;IAC3B,IAAID,eAAe,CAACE,GAAG,CAACD,CAAC,CAAC,EAAE;IAE5B,IAAM9C,OAAO,GAAG;MACdV,MAAM,EAAE,CAACgD,KAAK,CAAC;MACfjC,QAAQ,EAAEZ,kBAAkB,CAAC6C,KAAK;IACpC,CAAC;;IAED;IACAhD,MAAM,CAACzC,OAAO,CAAC,UAACmG,UAAU,EAAEC,CAAC,EAAK;MAChC,IAAIH,CAAC,KAAKG,CAAC,IAAI,CAACJ,eAAe,CAACE,GAAG,CAACE,CAAC,CAAC,EAAE;QACtC,IAAM9B,UAAU,GAAGT,2BAA2B,CAC5CjB,kBAAkB,CAAC6C,KAAK,CAAC,EACzB7C,kBAAkB,CAACuD,UAAU,CAC/B,CAAC;QAED,IAAI7B,UAAU,GAAG,GAAG,EAAE;UACpBnB,OAAO,CAACV,MAAM,CAAC7B,IAAI,CAACuF,UAAU,CAAC;UAC/BhD,OAAO,CAACK,QAAQ,GAAA6C,kBAAA,CAAO,IAAInD,GAAG,IAAA9C,MAAA,CAAAiG,kBAAA,CACzBlD,OAAO,CAACK,QAAQ,GAAA6C,kBAAA,CAChBzD,kBAAkB,CAACuD,UAAU,CAAC,EAClC,CAAC,CAAC;UACHH,eAAe,CAACJ,GAAG,CAACQ,CAAC,CAAC;QACxB;MACF;IACF,CAAC,CAAC;IAEFL,QAAQ,CAACnF,IAAI,CAACuC,OAAO,CAAC;IACtB6C,eAAe,CAACJ,GAAG,CAACK,CAAC,CAAC;EACxB,CAAC,CAAC;EAEF,OAAOF,QAAQ;AACjB;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASlC,2BAA2BA,CAACyC,SAAS,EAAEC,SAAS,EAAE;EACzD,IAAMC,IAAI,GAAG,IAAItD,GAAG,CAACoD,SAAS,CAAC;EAC/B,IAAMG,IAAI,GAAG,IAAIvD,GAAG,CAACqD,SAAS,CAAC;;EAE/B;EACA,IAAMG,YAAY,GAAG,IAAIxD,GAAG,CAACmD,kBAAA,CAAIG,IAAI,EAAE/C,MAAM,CAAC,UAAAkD,CAAC;IAAA,OAAIF,IAAI,CAACP,GAAG,CAACS,CAAC,CAAC;EAAA,EAAC,CAAC;EAChE,IAAMC,KAAK,GAAG,IAAI1D,GAAG,IAAA9C,MAAA,CAAAiG,kBAAA,CAAKG,IAAI,GAAAH,kBAAA,CAAKI,IAAI,EAAC,CAAC;;EAEzC;EACA,IAAII,YAAY,GAAG,CAAC;EACpBP,SAAS,CAACtG,OAAO,CAAC,UAAA8G,QAAQ,EAAI;IAC5BP,SAAS,CAACvG,OAAO,CAAC,UAAA+G,QAAQ,EAAI;MAC5B,IAAIC,WAAW,CAACF,QAAQ,EAAEC,QAAQ,CAAC,EAAE;QACnCF,YAAY,IAAI,CAAC;MACnB;IACF,CAAC,CAAC;EACJ,CAAC,CAAC;EAEF,IAAMI,iBAAiB,GAAGP,YAAY,CAACQ,IAAI,GAAGN,KAAK,CAACM,IAAI;EACxD,IAAMC,sBAAsB,GAAGN,YAAY,GAAG7F,IAAI,CAACC,GAAG,CAACqF,SAAS,CAAC/F,MAAM,EAAEgG,SAAS,CAAChG,MAAM,CAAC;EAE1F,OAAQ0G,iBAAiB,GAAG,GAAG,GAAKE,sBAAsB,GAAG,GAAI;AACnE;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA,SAASH,WAAWA,CAACI,KAAK,EAAEC,KAAK,EAAE;EACjC,OAAOC,WAAW,CAAC3D,IAAI,CAAC,UAAAN,GAAG;IAAA,OACzBA,GAAG,CAAC6C,GAAG,CAACkB,KAAK,CAACrH,WAAW,CAAC,CAAC,CAAC,IAAIsD,GAAG,CAAC6C,GAAG,CAACmB,KAAK,CAACtH,WAAW,CAAC,CAAC,CAAC;EAAA,CAC9D,CAAC;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASyE,wBAAwBA,CAACzB,eAAe,EAAE;EACjD,IAAMwE,aAAa,GAAGxE,eAAe,CAACmE,IAAI;EAC1C,IAAMM,eAAe,GAAG9C,KAAK,CAACC,IAAI,CAAC5B,eAAe,CAACtB,MAAM,CAAC,CAAC,CAAC,CACzDgC,MAAM,CAAC,UAAAN,OAAO;IAAA,OAAIA,OAAO,CAACG,OAAO;EAAA,EAAC,CAAC/C,MAAM;EAE5C,IAAMkH,kBAAkB,GAAID,eAAe,GAAGD,aAAa,GAAI,GAAG;;EAElE;EACA,IAAMG,oBAAoB,GAAGhD,KAAK,CAACC,IAAI,CAAC5B,eAAe,CAACtB,MAAM,CAAC,CAAC,CAAC,CAC9DoD,GAAG,CAAC,UAAA1B,OAAO;IAAA,OAAIA,OAAO,CAACI,iBAAiB,CAAChD,MAAM;EAAA,EAAC;EAEnD,IAAMoH,sBAAsB,GAAGD,oBAAoB,CAAChG,MAAM,CAAC,UAACC,CAAC,EAAEC,CAAC;IAAA,OAAKD,CAAC,GAAGC,CAAC;EAAA,GAAE,CAAC,CAAC,GAAG2F,aAAa;EAC9F,IAAMK,eAAe,GAAGC,0BAA0B,CAACH,oBAAoB,CAAC;EAExE,OAAO;IACLH,aAAa,EAAbA,aAAa;IACbC,eAAe,EAAfA,eAAe;IACfC,kBAAkB,EAAlBA,kBAAkB;IAClBK,YAAY,EAAE;MACZH,sBAAsB,EAAtBA,sBAAsB;MACtBC,eAAe,EAAfA,eAAe;MACfG,UAAU,EAAEH,eAAe,GAAGD,sBAAsB,GAAG;IACzD;EACF,CAAC;AACH;;AAEA;AACA;AACA;AACA;AACA;AACA,SAASE,0BAA0BA,CAACpG,MAAM,EAAE;EAC1C,IAAMuG,GAAG,GAAGvG,MAAM,CAACC,MAAM,CAAC,UAACC,CAAC,EAAEC,CAAC;IAAA,OAAKD,CAAC,GAAGC,CAAC;EAAA,GAAE,CAAC,CAAC,GAAGH,MAAM,CAAClB,MAAM;EAC7D,IAAM0H,WAAW,GAAGxG,MAAM,CAACoD,GAAG,CAAC,UAAAqD,KAAK;IAAA,OAAIlH,IAAI,CAACmH,GAAG,CAACD,KAAK,GAAGF,GAAG,EAAE,CAAC,CAAC;EAAA,EAAC;EACjE,OAAOhH,IAAI,CAACoH,IAAI,CAACH,WAAW,CAACvG,MAAM,CAAC,UAACC,CAAC,EAAEC,CAAC;IAAA,OAAKD,CAAC,GAAGC,CAAC;EAAA,GAAE,CAAC,CAAC,GAAGH,MAAM,CAAClB,MAAM,CAAC;AAC1E;;AAEA;AACA,IAAMsF,cAAc,GAAG,IAAI3C,GAAG,CAAC,CAC7B,WAAW,EAAE,UAAU,EAAE,WAAW,EAAE,aAAa,EACnD,gBAAgB,EAAE,cAAc,EAAE,gBAAgB,EAAE,cAAc,CACnE,CAAC;AAEF,IAAMoE,WAAW,GAAG,CAClB,IAAIpE,GAAG,CAAC,CAAC,UAAU,EAAE,MAAM,EAAE,MAAM,EAAE,QAAQ,CAAC,CAAC,EAC/C,IAAIA,GAAG,CAAC,CAAC,UAAU,EAAE,SAAS,EAAE,QAAQ,EAAE,QAAQ,CAAC,CAAC,EACpD,IAAIA,GAAG,CAAC,CAAC,SAAS,EAAE,SAAS,EAAE,aAAa,EAAE,OAAO,CAAC,CAAC,EACvD,IAAIA,GAAG,CAAC,CAAC,QAAQ,EAAE,UAAU,EAAE,SAAS,EAAE,SAAS,CAAC;AACpD;AAAA,CACD","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}